{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from random import shuffle, seed\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from utils import Config, safe_pickle_dump\n",
    "\n",
    "seed(1337)\n",
    "max_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read database\n",
    "db = pickle.load(open(Config.db_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all text files for all papers into memory\n",
    "txt_paths, pids = [], []\n",
    "n = 0\n",
    "for pid,j in db.items():\n",
    "  n += 1\n",
    "  idvv = '%sv%d' % (j['_rawid'], j['_version'])\n",
    "  txt_path = os.path.join('data', 'txt', idvv) + '.pdf.txt'\n",
    "  if os.path.isfile(txt_path): # some pdfs dont translate to txt\n",
    "    with open(txt_path, 'r') as f:\n",
    "      try:\n",
    "        txt = f.read()\n",
    "      except Exception as e:\n",
    "        print(\"Error reading file: %s\" % txt_path)\n",
    "        print(e)\n",
    "        #raise SystemExit(0)\n",
    "        with open(\"analyze.log\", \"a\") as myfile:\n",
    "          myfile.write(\"Error reading file: %s\" % txt_path)\n",
    "    if len(txt) > 1000 and len(txt) < 500000: # 500K is VERY conservative upper bound\n",
    "      txt_paths.append(txt_path) # todo later: maybe filter or something some of them\n",
    "      pids.append(idvv)\n",
    "      #print(\"read %d/%d (%s) with %d chars\" % (n, len(db), idvv, len(txt)))\n",
    "      print('.', end='')\n",
    "    else:\n",
    "      print(\"\\n Skipped %d/%d (%s) with %d chars: suspicious!\" % (n, len(db), idvv, len(txt)))\n",
    "  else:\n",
    "    #print(\"\\n Could not find %s in txt folder.\" % (txt_path, ))\n",
    "    print('x', end='')\n",
    "print(\"in total read in %d text files out of %d db entries.\" % (len(txt_paths), len(db)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tfidf vectors with scikits\n",
    "v = TfidfVectorizer(input='content',\n",
    "        encoding='utf-8', decode_error='replace', strip_accents='unicode',\n",
    "        lowercase=True, analyzer='word', stop_words='english',\n",
    "        token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b',\n",
    "        ngram_range=(1, 2), max_features = max_features,\n",
    "        norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "        max_df=1.0, min_df=1)\n",
    "\n",
    "# create an iterator object to conserve memory\n",
    "def make_corpus(paths):\n",
    "  for p in paths:\n",
    "    with open(p, 'r') as f:\n",
    "      try:\n",
    "        txt = f.read()\n",
    "      except Exception as e:\n",
    "        print(\"Error reading file: %s\" % txt_path)\n",
    "        print(e)\n",
    "        with open(\"analyze.log\", \"a\") as myfile:\n",
    "          myfile.write(\"Error making corpus: %s\" % p)\n",
    "    yield txt\n",
    "\n",
    "\n",
    "max_train = 500 # max number of tfidf training documents (chosen randomly), for memory efficiency\n",
    "\n",
    "# train\n",
    "train_txt_paths = list(txt_paths) # duplicate\n",
    "shuffle(train_txt_paths) # shuffle\n",
    "train_txt_paths = train_txt_paths[:min(len(train_txt_paths), max_train)] # crop\n",
    "print(\"training on %d documents...\" % (len(train_txt_paths), ))\n",
    "train_corpus = make_corpus(train_txt_paths)\n",
    "v.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "print(\"transforming %d documents...\" % (len(txt_paths), ))\n",
    "corpus = make_corpus(txt_paths)\n",
    "X = v.transform(corpus)\n",
    "print(v.vocabulary_)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write full matrix out\n",
    "out = {}\n",
    "out['X'] = X # this one is heavy!\n",
    "print(\"writing\", Config.tfidf_path)\n",
    "safe_pickle_dump(out, Config.tfidf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing lighter metadata information into a separate (smaller) file\n",
    "out = {}\n",
    "out['vocab'] = v.vocabulary_\n",
    "out['idf'] = v._tfidf.idf_\n",
    "out['pids'] = pids # a full idvv string (id and version number)\n",
    "out['ptoi'] = { x:i for i,x in enumerate(pids) } # pid to ix in X mapping\n",
    "print(\"writing\", Config.meta_path)\n",
    "safe_pickle_dump(out, Config.meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"precomputing nearest neighbor queries in batches...\")\n",
    "sim_dict = {}\n",
    "batch_size = 200\n",
    "for i in range(0, len(pids), batch_size):\n",
    "    i1 = min(len(pids), i+batch_size)\n",
    "    xquery = X[i:i1] # BxD\n",
    "    ds = -(X.dot(xquery.T)).toarray() #NxD * DxB => NxB\n",
    "    IX = np.argsort(ds, axis=0) # NxB\n",
    "    for j in range(i1-i):\n",
    "        print(i)\n",
    "        print(j)\n",
    "        sim_dict[pids[i+j]] = [pids[q] for q in list(IX[:50,j])]\n",
    "    print('%d/%d...' % (i, len(pids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing\", Config.sim_path)\n",
    "safe_pickle_dump(sim_dict, Config.sim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "i1 = min(len(pids), i+batch_size)\n",
    "xquery = X[i:i1] # BxD\n",
    "ds = -(X.dot(xquery.T)).toarray() #NxD * DxB => NxB\n",
    "#ds = -np.asarray(np.dot(X, xquery.T)) #NxD * DxB => NxB\n",
    "IX = np.argsort(ds, axis=0) # NxB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "sim_dict[pids[i+j]] = [pids[q] for q in list(IX[:50,j])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids[i+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IX[:50, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
